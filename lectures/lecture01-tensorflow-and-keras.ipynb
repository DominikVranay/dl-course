{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Deep Learning with TensorFlow and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's class we'll look at how easy it is to implement a neural net using the framework TensorFlow and the library Keras in Python. We'll also explain the basic building blocks of a neural net, such as neuron, layer, activation function, loss function, and how we train neural nets.\n",
    "\n",
    "Sources:\n",
    "* [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/)\n",
    "* [Keras documentation](https://keras.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importing TensorFlow and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement and train our first neural nets, we first need to have TensorFlow and Keras installed and imported. A manual on installing these tools can be found in **Lab 0**.\n",
    "\n",
    "If you already have the tools installed, we can import them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some warnings pop out, don't be alarmed: they're only caused by the frequent updates of various libraries which in turn might use some of each other's deprecated methods.\n",
    "\n",
    "If you could import tensorflow successfully, we can continue with importing Keras and other useful libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pandas` is a useful library for working with datasets. It enables us to use universal indexing and provides access to values through keys.\n",
    "\n",
    "The library `sklearn` offers methods related to machine learning. From these options, we will use the class `LabelEncoder` which can vectorize our non-numerical values (such as class labels). The method `train_test_split` will be used to automatically create a training and testing set.\n",
    "\n",
    "Finally, we import all the classes that we will use from `Keras`:\n",
    "* `Sequential` is a type of neural network, and represents simple feed-forward neural nets\n",
    "* `Dense` is a fully connected layer where each neuron in the layer is connected to all neurons from the previous layer\n",
    "* `Adam` is a frequently used optimizer that speeds up the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building a Perceptron in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron is an algorithm for supervised learning of binary classifiers and can be represented as a single neuron. This provides us with the perfect opportunity to look at how neurons work and how they are implemented in TensorFlow at a low level. Don't worry, Keras adds some extra layers of abstraction that will make it easy to build neural nets.\n",
    "\n",
    "Let's first look at the structure of a neuron:\n",
    "![Structure of a neuron](figures/lab1-neuron.png)\n",
    "\n",
    "A neuron is a computational unit that is built up from a number of simple operations. First, it calculates a weighted sum of all inputs, then it adds a bias, and finally applies an activation function. Some definitions add an output function that usually returns the same value as its input and, therefore, we will not consider it here.\n",
    "\n",
    "We will now build a simple neuron and test its functionality by predefining all its weights to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple neuron with two input nodes\n",
    "def my_neuron(input_vals):\n",
    "    # define some arbitrary weights for the two input values\n",
    "    W = tf.Variable(tf.ones((2, 1)))\n",
    "\n",
    "    # define the bias of the neuron\n",
    "    b = tf.Variable(1, dtype=tf.float32)\n",
    "\n",
    "    # compute weighted sum (hint: check out tf.matmul)\n",
    "    z = tf.matmul(input_vals, W) + b\n",
    "    print(z)\n",
    "\n",
    "    # apply the sigmoid activation function (hint: use tf.sigmoid)\n",
    "    output = tf.sigmoid(z)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize the weights to 1 by creating a `Variable` - in TensorFlow, values can be saved into variables or constants. We call a method similar to `numpy`'s `ones` method: it creates an array of ones with the given shape (2 inputs, 1 output). We then initialize the bias to 1 as well.\n",
    "\n",
    "The method `matmul` is used to calculate the weighted sum (or dot product) to which we then add the bias. So we can check the result of the computation, we print `z`. Since in TensorFlow all computation is done through tensors, the result of the print statement will be a string representation of the particular tensor.\n",
    "\n",
    "Finally, we call the `sigmoid` method to represent the activation function sigmoid, which is one of the most popular ones for classification tasks.\n",
    "\n",
    "We can test our neuron by giving it some sample inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = tf.constant([[2, 3]], shape=(1, 2), dtype=tf.float32)\n",
    "\n",
    "# if you've done everything correctly, this should give you a tensor with value 0.9975274\n",
    "result = my_neuron(sample_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, the result of the print statement will be a string representation of the tensor and should look similar to this:\n",
    "\n",
    "```\n",
    "tf.Tensor([[6.]], shape=(1, 1), dtype=float32)\n",
    "tf.Tensor([[0.9975274]], shape=(1, 1), dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a Neural Net with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've taken a look under the hood of TensorFlow, we'll look at how easy it is to build a neural net from scratch with Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first task is to load the dataset using `pandas`. For the purposes of this lab, we'll be working with the Iris dataset which you can download [here](https://archive.ics.uci.edu/ml/datasets/Iris)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"iris.csv\")\n",
    "\n",
    "X = dataset.iloc[:, :4].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into the lists of inputs (first four columns) and outputs (last column). The output now contains layers as strings, which are unfitted for use with neural nets. Therefore, we vectorize them, meaning that we will change them into vectors with a length equal to the number of classes. All vectors will contain zeroes and a single 1 whose position represents the class (1 0 0 for class one, 0 1 0 for class two and 0 0 1 for class three)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y1 = encoder.fit_transform(y)\n",
    "\n",
    "Y = pd.get_dummies(y1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our vectorized outputs, we can split our dataset into training and testing sets. To this end, we will use the method `train_test_split` from `sklearn` which has the following main parameters:\n",
    "* list of inputs\n",
    "* list of outputs\n",
    "* test_size - the size of the testing set (in percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build our model. We first create an empty `Sequential` model (for model functionality check the [documentation](https://keras.io/models/model/)) and then we add fully connected layers to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(10, input_shape=(4,), activation='tanh'))\n",
    "model.add(Dense(8, activation='tanh'))\n",
    "model.add(Dense(6, activation='tanh'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the constructor of layers has a number of parameters. For the first added layer, you must specify the input size (the number of inputs from the dataset). For all layers, you must specify the number of neurons (first parameter) and you should also specify the activation function (the list of all available activation functions can be found [here](https://keras.io/activations/) but you can also create your own).\n",
    "\n",
    "If you want to check out the structure of the model, you can do so by calling the method `summary`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference between TensorFlow and PyTorch is that while PyTorch has dynamically compiled models (meaning that you can change the model during runtime), with TensorFlow you must first compile the model. We'll do so in the next step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.04), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main parameters are the following:\n",
    "* optimizer - the optimizer used to speed up the training process and the rate of convergence ([list of optimizers](https://keras.io/optimizers/))\n",
    "* loss - the loss function used; `categorical_crossentropy` is usually used for multi-class classification ([list of loss functions in Keras](https://keras.io/losses/))\n",
    "* metrics - the metrics shown during training to evaluate the performance of the model ([list of supported metrics](https://keras.io/metrics/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, you must call the method `fit` by providing the training inputs and output and the number of epochs. For further parameters, please check out the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As `fit` serves for training the model, `predict` can be used to ge the predicted output for a given input or list of inputs like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to evaluate the performance of your model using further metrics, you can do so using `numpy` and `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Y_test_class = np.argmax(Y_test, axis=1)\n",
    "Y_pred_class = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(Y_test_class, Y_pred_class))\n",
    "print(confusion_matrix(Y_test_class, Y_pred_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
