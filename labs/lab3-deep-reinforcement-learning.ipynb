{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Deep reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na dnešnom cvičení si vytvoríte vlastného agenta, ktorý pomocou reinforcement learningu sa naučí hrať jednoduché počítačové hry. Ak nepoznáte základy reinforcement learningu, odporúčame si naštudovať niektoré články:\n",
    "\n",
    "[A Beginner's Guide to Deep Reinforcement Learning](https://skymind.ai/wiki/deep-reinforcement-learning)\n",
    "\n",
    "[An Introduction to Deep Reinforcement Learning](https://arxiv.org/pdf/1811.12560.pdf)\n",
    "\n",
    "[Introduction to Deep Reinforcement Learning](https://medium.com/@jonathan_hui/rl-introduction-to-deep-reinforcement-learning-35c25e04c199)\n",
    "\n",
    "Počas cvičenia budeme využívať knižnicu [Gym od OpenAI](https://gym.openai.com). Nainštalujete si ju pomocou príkazu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Vytvorenie agenta\n",
    "\n",
    "V prvom kroku riešenia si zadefinujeme nášho učiaceho sa agenta, ktorý bude využívať metódu Deep Q-learning. Kostru riešenia nájdete na [tomto odkaze](https://github.com/ianmagyar/dl-course/blob/master/labs/sources/lab3.py) (zdrojom kódu je [Keon's blog](https://keon.io/deep-q-learning/)).\n",
    "\n",
    "V skripte máte zadefinovanú triedu `DQNAgent` s konštruktorom, s funkciami pre uloženie a načítanie modelu, a funkciu `replay`, ktorá je zodpovedná za samotné trénovanie modelu. Prečítajte si funkcie v triede `DQNAgent`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Zapamätanie kroku\n",
    "\n",
    "Aby sme dosiahli efektívne učenie a aby sme predišli prepisovaniu predošlých správaní, agent má svoju pamäť (premenná `memory`). Opravte funkciu `remember` tak, aby do pamäte uložil údaje o aktuálnej skúsenosti, ktoré funkcia dostane ako parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Výber akcie\n",
    "\n",
    "Jedna z najdôležitejších funkcií reinforcement learning agentu je schopnosť vybrať akciu, ktorá má najväčšiu očakávanú hodnotu. Problém s takým greedy prístupom je však to, že zabráni hľadaniu alternatívnych správaní, ktoré napokon môžu byť optimálnejšie, ako súčasné správanie modelu. Práve preto sa používajú tzv. e-greedy modely, ktoré vyberajú akcie náhodne s istou pravdepodonosťou. Túto pravdepodobnosť máme uloženú v premennej `epsilon`.\n",
    "\n",
    "Opravte funkciu act tak, aby vyberal akcie e-greedy spôsobom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Definícia modelu\n",
    "\n",
    "Neostáva nám nič iné iba zadefinovať deep model pre učenie. Model si môžete zadefinovať ako len chcete, alebo môžete použiť nasledovnú topológiu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_1 (Dense)              (None, 24)                120       \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 24)                600       \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 2)                 50        \n",
    "=================================================================\n",
    "Total params: 770\n",
    "Trainable params: 770\n",
    "Non-trainable params: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nezabudnite na to, že počet vstupných neurónov musí byť rovnaký ako počet parametrov opisujúcich stav hry (`state_size`), a počet výstupných neurónov sa rovná počtu možných akcií (`action_size`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testovanie agenta\n",
    "\n",
    "Náš agent otestujeme pomocou benchmark problému [CartPole](https://gym.openai.com/envs/CartPole-v1/). Vo funkcii `main` máte zadefinovaný spôsob testovania, naštudujte si kód a následne spustite testy.\n",
    "\n",
    "Zobrazí sa Vám výpis vo forme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "episode: 1/1000, score: 37, e: 0.9\n",
    "episode: 2/1000, score: 16, e: 0.83\n",
    "episode: 3/1000, score: 18, e: 0.76\n",
    "episode: 4/1000, score: 8, e: 0.73\n",
    "episode: 5/1000, score: 11, e: 0.69\n",
    "episode: 6/1000, score: 10, e: 0.66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Čo nie je až také zaujímavé. Práve preto odkomentujte riadok `env.render()` vo funkcii `main`, čo Vám umožní sledovať výkon Vášho modelu priamo počas trénovania!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Začnite experimentovať s rôznymi topológiami siete, s rôznymi aktivačnými funkciami, optimizátorom, learning rateom, atď. Následne si môžete vyskúšať Váš model v inom prostredí, iba opravte riadok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a načítajte iné prostredie zo zoznamu [OpenAI Gym](https://gym.openai.com/envs/#classic_control)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ukladanie a načítanie modelu\n",
    "\n",
    "V triede `DQNAgent` sú definované funkcie `save` a `load`, ktoré slúžia na ukladanie a načítanie váh siete. Očakávajú však že model bude mať vždy rovnakú topológiu. Keras nám ale umožňuje ukladanie a načítanie samotnú topológiu modelu do tvaru JSON. Vašou poslednou úlohou je teda pridať funkcie `save_model` a `load_model`, ktoré budú pracovať so samotnou sieťou nielen váhami. Následne opravte funkciu `_build_model`, aby načítal model priamo zo súboru.\n",
    "\n",
    "Môžete sa inšpirovať kódom [tu](https://machinelearningmastery.com/save-load-keras-deep-learning-models/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
