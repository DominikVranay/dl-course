{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning\n",
    "\n",
    "Up until now you have worked only with supervised and unsupervised learning. Today we will have a look at a new technique in machine learning, called reinforcement learning (RL). This approach is applicable, when, although, the task requires consideration of feedback from the environment, the system may not rely on a supervisor to critically assess the output. The learning signal is based on the reward given by the environment to the agent.\n",
    "\n",
    "It is meant to be a straightforward framing of the problem of learning from interaction to achieve the goal by maximizing the collected reward of the course of interacting with the environment.\n",
    "\n",
    "![reinforcement learning](https://www.researchgate.net/profile/Roohollah_Amiri/publication/323867253/figure/fig2/AS:606095550738432@1521515848671/Reinforcement-Learning-Agent-and-Environment.png)\n",
    "\n",
    "The RL algorithm consists of several parts:\n",
    "\n",
    "- environment\n",
    "- agent - the actor within the environment\n",
    "- state - set of parameters describing the environment at time t\n",
    "- action - action to be taken by the actor\n",
    "- reward - feedback given by the environment\n",
    "- (policy)\n",
    "- (value function)\n",
    "\n",
    "Unlike (un)supervised learning, RL does not have any data in advance. The agent acts within the environment, explores it at first to acquire knowledge, then uses said knowledge to exploit the environment for its own gain (tries to maximize reward). \n",
    "\n",
    "![mouse and cheese](https://miro.medium.com/max/1448/0*WH2kRYzeDx1zTdzI.png)\n",
    "\n",
    "The simplest approach would be to let an algorithm learn the mapping between a state and its actions, which yields the highest reward (either immidiate or long-term). This is called Q-learning and serves as a basis for more complex algorithms besides is obvious use-case in simple problems. Q-table, which represents this state-action mapping is explicit - the algorithm has to try most (preferably all) combinations to learn properly. Obviously, some areas are going to be visited more often (and are likely more important). As long, as the state and action spaces are discrete and finite (and kept small enough), we can use Q-learning. Although, with growing sizes, this becomes at first impractical, later outright impossible (due to memory and computational limitations).\n",
    "\n",
    "This Q-table, which is a learned representation of reward function can be replaced by a neural network. \n",
    "Today we will be using Deep Q network.\n",
    "\n",
    "It has been able to successfuly learn to play various games, including chess and Atari breakout.\n",
    "\n",
    "Further discussion (too long to be written out here):\n",
    "\n",
    "- exploration vs exploitation\n",
    "- value-based vs policy-based\n",
    "- model-free vs model-based\n",
    "- actor-critic\n",
    "\n",
    "\n",
    "We will be using OpenAI Gym (with PyTorch) as our playground today. You can install it by entering:\n",
    "```\n",
    "pip install gym\n",
    "pip install box2d-py\n",
    "```\n",
    "in your (virtual) environment.\n",
    "\n",
    "official documentation is available here: https://gym.openai.com/\n",
    "\n",
    "To check, whether everything is installed properly, try running one of available environments (just ignore the warning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The network itself is just a simple fully connected network with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, \n",
    "            n_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions,\n",
    "            max_mem_size=750000, eps_end=0.05, eps_dec=5e-4):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "        self.iter_cntr = 0\n",
    "        self.replace_target = 100\n",
    "\n",
    "        self.Q_eval = DeepQNetwork(lr, n_actions=n_actions, input_dims=input_dims,\n",
    "                                    fc1_dims=256, fc2_dims=256)\n",
    "        self.Q_next = DeepQNetwork(lr, n_actions=n_actions, input_dims=input_dims,\n",
    "                                    fc1_dims=64, fc2_dims=64)\n",
    "\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, terminal):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = terminal\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([observation]).to(self.Q_eval.device)\n",
    "            actions = self.Q_eval.forward(state)\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        \n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        action_batch = self.action_memory[batch]\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "\n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "\n",
    "        q_target = reward_batch + self.gamma*T.max(q_next,dim=1)[0]\n",
    "\n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "\n",
    "        self.iter_cntr += 1\n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min \\\n",
    "                       else self.eps_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0  score  0\n",
      "episode  1  score  -116.13652522049954\n",
      "episode  2  score  -209.65344247587473\n",
      "episode  3  score  -153.50514077670348\n",
      "episode  4  score  -94.7009321839665\n",
      "episode  5  score  -191.47330947532961\n",
      "episode  6  score  -15.2539408106704\n",
      "episode  7  score  -92.0021286631884\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "agent = Agent(gamma=0.99, epsilon=1.0, batch_size=64, n_actions=4, input_dims=[8], lr=0.005)\n",
    "scores = []\n",
    "eps_history = []\n",
    "n_games = 1000\n",
    "score = 0\n",
    "for i in range(n_games):\n",
    "    if i % 10 == 0 and i > 0:\n",
    "        avg_score = np.mean(scores[max(0, i-10):(i+1)])\n",
    "        print('episode ', i, ' score ', score, ' average score %.3f' %avg_score, ' epsilon %.3f' %agent.epsilon)\n",
    "    else:\n",
    "        print('episode ', i, ' score ', score)\n",
    "    score = 0\n",
    "    eps_history.append(agent.epsilon)\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.store_transition(observation, action, reward, observation_, done)\n",
    "        agent.learn()\n",
    "        observation = observation_\n",
    "    scores.append(score)\n",
    "    env.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
